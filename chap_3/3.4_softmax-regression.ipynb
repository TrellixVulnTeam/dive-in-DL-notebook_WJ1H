{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 线性回归 >> 输出连续值 >> 回归 （输出值的数目为1） >> 使用MSE损失函数\n",
    "### softmax回归 >> 输出离散值 >> 分类 （输出值的数目为n） >> 使用交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 同样都是进行特征与权重的线性叠加\n",
    "### 两者的一个主要不同点在于，softmax回归的输出值个数等于标签的类别数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax都解决了哪些问题\n",
    "\n",
    "答： 在没有加入softmax（作为激活函数）的情况下，仅依赖输出值作为某个输入值在权重的作用下的真实反映这样似乎不能反映出真实的情况， 如三个类cat, dog, hense的置信度为0.1, 10, 0.1, 可以轻易判断其为dog, 但这个置信度相对于其他置信度相当不稳定。如一旦三者的置信度变为$10^3, 10, 10^3$呢，则 **没有一个具体范围对其进行约束** 。\n",
    "\n",
    "解决的第二个问题是，输出的（范围不确定的）置信度与真实标签之间的 **误差很难衡量** 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax的具体表示方法\n",
    "\n",
    "若$$o_1 = x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,$$\n",
    "$$o_2 = x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,$$\n",
    "$$o_3 = x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3,$$\n",
    "\n",
    "中间无隐藏层，$o_1, o_2, o_3$为输出层，经过softmax处理有，\n",
    "\n",
    "$$ \\hat{y}_1, \\hat{y}_2,\\hat{y}_3 = \\text{softmax}(o_1, o_2, o_3)$$, \n",
    "$$\n",
    "\\hat{y}_1 = \\frac{ \\exp(o_1)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad\n",
    "\\hat{y}_2 = \\frac{ \\exp(o_2)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad\n",
    "\\hat{y}_3 = \\frac{ \\exp(o_3)}{\\sum_{i=1}^3 \\exp(o_i)}.\n",
    "$$\n",
    "\n",
    "从中可以看出，\n",
    "\n",
    "1. $ \\hat{y}_1+ \\hat{y}_2+\\hat{y}_3 =1 $, 且$ 0 \\lt\\hat{y}_1, \\hat{y}_2,\\hat{y}_3 \\lt 1 $， 这个概率是合法的;\n",
    "\n",
    "2. $ \\underset{i}{\\arg\\max} o_i = \\underset{i}{\\arg\\max} \\hat{y}_i $ ,经过softmax运算后不会影响他的预测概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵损失函数的表达式\n",
    "\n",
    "$$ H\\left( \\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)} \\right)  = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},$$\n",
    "\n",
    "其中$i$表示第$i$个样本，$q$为总类别数，$j$指向第$j$类，$y_j^{(i)}$非1即0.在向量$\\boldsymbol y^{(i)}$中，只有第$y^{(i)}$个元素为1, 其他元素为零，因此$ H\\left( \\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)} \\right)  = -\\log \\hat{y}_{y^{(i)}}^{(i)}$. 从中可以看出，交叉熵只关注正确类别的预测概率，只要这个值足够大，就可以确保分类结果正确。\n",
    "\n",
    "扩展到训练样本数为$n$的情况， 则有\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) \\  \\to \\  \n",
    "\\ell(\\boldsymbol{\\Theta}) = -\\frac{1}{n}  \\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)} \\  \\to \\   \\exp(-n\\ell(\\boldsymbol{\\Theta}))=\\prod_{i=1}^n \\hat y_{y^{(i)}}^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。(softmax回归在线性回归的基础上加入了softmax运算，并且改变了损失函数，其他都一致)\n",
    "* softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数。\n",
    "* 交叉熵适合衡量两个概率分布的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
