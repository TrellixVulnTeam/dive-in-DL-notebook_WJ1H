{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NiN网络的提出初衷是：\n",
    "# 1. LeNet, AlexNet, VGG net都是conv模块（抽取空间特征）＋FC模块(分类)．\n",
    "# 2. AlexNet, VGG net都是在ＬeNet的基础上在宽度（channels)和深度（layers）上下工夫\n",
    "# 3. NiN则尝试使用多个conv+'fc'层来构建深层网络．\n",
    "\n",
    "## 具体做法：\n",
    "# 1. conv模块的输入输出都是４维，fc模块的输入是４维，输出是２维\n",
    "# 2. 如果要在FC模块后面再加上卷积层，那么要先将２维上升到４维\n",
    "# 3. 由于chap5.3介绍１x1可以视为fc层，这样避免了降维和多余的升维．\n",
    "# 4. 因此，使用１x1conv替代fc层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NiN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('../d2lzh/')\n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NiN model(NiN是在AlexNet后不久提出，故他们的内部大致相当)\n",
    "# 不同之处：\n",
    "# 1. AlexNet中的FC模块被替换为了nin_block中的out_channels(10类)\n",
    "# 2. 随后的10个feature maps被全局池化层变成了１个包含10个元素的行向量．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class GlobalAvgPool2d(nn.Module): # 全局平均池化层\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d( x, kernel_size=x.size()[2:] )\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    \n",
    "    # 这部分代码替代了ＦＣ模块\n",
    "    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(),  # 输出为４维(batchsize, channel=10, h=1, w=1)\n",
    "    d2l.FlattenLayer(), # 度压缩 (batchsize, channel=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape: torch.Size([1, 96, 54, 54])\n",
      "1 output shape: torch.Size([1, 96, 26, 26])\n",
      "2 output shape: torch.Size([1, 256, 26, 26])\n",
      "3 output shape: torch.Size([1, 256, 12, 12])\n",
      "4 output shape: torch.Size([1, 384, 12, 12])\n",
      "5 output shape: torch.Size([1, 384, 5, 5])\n",
      "6 output shape: torch.Size([1, 384, 5, 5])\n",
      "7 output shape: torch.Size([1, 10, 5, 5])\n",
      "8 output shape: torch.Size([1, 10, 1, 1])\n",
      "9 output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 调试net,查看各layer的shape\n",
    "Ｘ = torch.rand(1, 1, 224, 224)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 获取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.2928, train_acc 0.538, test acc 0.796, time 80.1 sec\n",
      "epoch 2, loss 0.5473, train_acc 0.801, test acc 0.821, time 80.6 sec\n",
      "epoch 3, loss 0.4770, train_acc 0.825, test acc 0.838, time 80.8 sec\n",
      "epoch 4, loss 0.4406, train_acc 0.837, test acc 0.855, time 81.7 sec\n",
      "epoch 5, loss 0.4092, train_acc 0.849, test acc 0.872, time 80.8 sec\n",
      "epoch 6, loss 0.3816, train_acc 0.859, test acc 0.870, time 80.8 sec\n",
      "epoch 7, loss 0.3568, train_acc 0.869, test acc 0.885, time 80.8 sec\n",
      "epoch 8, loss 0.3345, train_acc 0.876, test acc 0.887, time 81.5 sec\n",
      "epoch 9, loss 0.3239, train_acc 0.882, test acc 0.897, time 80.8 sec\n",
      "epoch 10, loss 0.3050, train_acc 0.889, test acc 0.905, time 80.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.002, 10 # 学习率增加\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter=train_iter, test_iter=test_iter,\n",
    "             batch_size=batch_size, optimizer=optimizer, \n",
    "              device=device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training on  cuda\n",
    "# epoch 1, loss 1.2928, train_acc 0.538, test acc 0.796, time 80.1 sec\n",
    "# epoch 2, loss 0.5473, train_acc 0.801, test acc 0.821, time 80.6 sec\n",
    "# epoch 3, loss 0.4770, train_acc 0.825, test acc 0.838, time 80.8 sec\n",
    "# epoch 4, loss 0.4406, train_acc 0.837, test acc 0.855, time 81.7 sec\n",
    "# epoch 5, loss 0.4092, train_acc 0.849, test acc 0.872, time 80.8 sec\n",
    "# epoch 6, loss 0.3816, train_acc 0.859, test acc 0.870, time 80.8 sec\n",
    "# epoch 7, loss 0.3568, train_acc 0.869, test acc 0.885, time 80.8 sec\n",
    "# epoch 8, loss 0.3345, train_acc 0.876, test acc 0.887, time 81.5 sec\n",
    "# epoch 9, loss 0.3239, train_acc 0.882, test acc 0.897, time 80.8 sec\n",
    "# epoch 10, loss 0.3050, train_acc 0.889, test acc 0.905, time 80.8 sec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
