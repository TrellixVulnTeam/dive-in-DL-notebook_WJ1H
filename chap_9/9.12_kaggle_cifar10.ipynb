{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 从原始cifar10数据开始，进行图像分类。\n",
    "\n",
    "# 1. 数据预处理\n",
    "### 读取标签文件\n",
    "### 根据标签文件，将文件名和文件label生成一个dict数据结构\n",
    "### 拆分train, val数据, 加载test数据\n",
    "### 增广\n",
    "### 生成train_iter, val_iter, test_iter\n",
    "### \n",
    "\n",
    "# 2. 建立模型\n",
    "# 3. 训练模型\n",
    "# 4. 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import相关包\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "import time\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../d2lzh/')\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 1.1下载数据集\n",
    "### 1.2解压数据集\n",
    "'''\n",
    "    ../data/kaggle_cifar10/train/[1-50000].png；\n",
    "    ../data/kaggle_cifar10/test/[1-300000].png；\n",
    "    ../data/kaggle_cifar10/trainLabels.csv。\n",
    "'''\n",
    "\n",
    "# 考虑到原文件数据太多，先抽取500张训练样本，5张测试样本，这样容易上手。\n",
    "'''\n",
    "    ../data/kaggle_cifar10/train_tiny/[1-500].png；\n",
    "    ../data/kaggle_cifar10/test/[1-5].png；\n",
    "    ../data/kaggle_cifar10/trainLabels.csv。\n",
    "'''\n",
    "\n",
    "demo = False\n",
    "# if demo:\n",
    "#     import zipfile\n",
    "#     for f in ['train_tiny.zip', 'test_tiny.zip', 'trainLables.csv.zip']:\n",
    "#         with zipfile.Zipfile('../data/kaggle_cifar10/' + f, 'r') as z:\n",
    "#             z.extractall('../data/kaggle_cifar10/')\n",
    "            \n",
    "\n",
    "### 1.3整理数据集\n",
    "def read_label_file(data_dir, label_file, train_dir, valid_ratio):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        读取label.csv文件，将数据集的大概信息（总数，每一类平均数等）整理出来。\n",
    "    Args:\n",
    "        data_dir: train， val数据集的路經\n",
    "        label_file: label.csv文件名\n",
    "        train_dir: train数据集的文件名：train 或 train_tiny\n",
    "        valid_ratio: 在提供的训练集中，分出用于验证的比例\n",
    "    Return:\n",
    "        n_train_per_label: 每一类的平均数。\n",
    "        idx_label: 字典结构，“文件名”及其对应的\"label\"\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict((int(idx), label) for idx, label in tokens)\n",
    "    \n",
    "    labels = set(idx_label.values())\n",
    "    n_train_valid = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    n_train = int(n_train_valid * (1 - valid_ratio))\n",
    "    n_train_per_label = n_train // len(labels)\n",
    "    assert 0 < n_train < n_train_valid\n",
    "    \n",
    "    return n_train_per_label, idx_label\n",
    "\n",
    "\n",
    "###　检查某路径下文件名是否存在，如无，在该路径下创建它\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.mkdir(os.path.join(*path))\n",
    "        \n",
    "def reorg_train_valid(data_dir, train_dir, input_dir, n_train_per_label, idx_label):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        将给定的train数据进行整理归类，划分好训练集和测试集．\n",
    "    Args:%%!\n",
    "        data_dir: \"数据的路径\"\n",
    "        train_dir:　\"train\" or \"train_tiny\"\n",
    "        input_dir: \"train_val_test\"\n",
    "        n_train_per_label： 每个label类至少要有多少张图片\n",
    "        idx_label: 字典结构，“文件名”及其对应的\"label\"\n",
    "    Return:\n",
    "        已经重新整理的文件：\n",
    "        data_dir/input_dir/train/label/xxx.jpg\n",
    "        data_dir/input_dir/valid/label/xxx.jpg\n",
    "    \"\"\"\n",
    "    \n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])  # 因为文件是按１,2,3.png...来命名的\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        \n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                   os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        \n",
    "        if label not in label_count or label_count[label] < n_train_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "            \n",
    "        else: \n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            \n",
    "            \n",
    "def reorg_test(data_dir, test_dir, input_dir):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        将给定的test数据放到自己定义的路径下面．\n",
    "    Args:%%!\n",
    "        data_dir: \"数据的路径\"\n",
    "        test_dir:　\"test\" or \"test_tiny\"\n",
    "        input_dir: \"train_val_test\"\"\n",
    "    Return:\n",
    "        已经重新整理的文件：\n",
    "        data_dir/input_dir/test/unknown/xxx.jpg\n",
    "    \"\"\"\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                   os.path.join(data_dir, input_dir, 'test', 'unknown'))\n",
    "        \n",
    "        \n",
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir,\n",
    "                      input_dir, valid_ratio):\n",
    "    n_train_per_label, idx_label = read_label_file(data_dir,\n",
    "                                                  label_file,\n",
    "                                                  train_dir,\n",
    "                                                  valid_ratio)\n",
    "    reorg_train_valid(data_dir, train_dir, input_dir, n_train_per_label, idx_label)\n",
    "    reorg_test(data_dir, test_dir, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if demo:\n",
    "    train_dir, test_dir =  'train_tiny', 'test_tiny'\n",
    "    batch_size = 1\n",
    "    label_file = 'trainLablesTiny.csv'\n",
    "else:\n",
    "    train_dir, test_dir, batch_size = 'train', 'test', 64\n",
    "    label_file = 'trainLabels.csv'\n",
    "\n",
    "data_dir = '../Datasets/cifar-10/'\n",
    "input_dir = 'train_valid_test'\n",
    "valid_ratio = 0.1\n",
    "# reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 数据增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(40),\n",
    "        torchvision.transforms.RandomResizedCrop(32, \n",
    "                                                scale=(0.64, 1.0),\n",
    "                                                ratio=(1.0, 1.0)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                        [0.2023, 0.1994, 0.2010])        \n",
    "    ])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            [0.4914, 0.4822, 0.4465],\n",
    "            [0.2023, 0.1994, 0.2010]\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 数据集准备\n",
    "\n",
    "# 读取数据集\n",
    "# train_data_path = os.path.join(data_dir, input_dir, 'train')\n",
    "# train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform_train)\n",
    "\n",
    "# valid_data_path = os.path.join(data_dir, input_dir, 'valid')\n",
    "# valid_data = torchvision.datasets.ImageFolder(root=valid_data_path, transform=transform_train)\n",
    "\n",
    "train_val_data_path = os.path.join(data_dir, input_dir, 'train_valid')\n",
    "train_val_data = torchvision.datasets.ImageFolder(root=train_val_data_path, transform=transform_train)\n",
    "\n",
    "test_data_path = os.path.join(data_dir, input_dir, 'test')\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform_test)\n",
    "\n",
    "# 根据batch size大小进行封装\n",
    "# train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "# valid_iter = torch.utils.data.DataLoader(valid_data, batch_size, shuffle=True)\n",
    "train_valid_iter = torch.utils.data.DataLoader(train_val_data, batch_size, shuffle=True)\n",
    "test_iter  = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型（resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = d2l.resnet18_cifar10(output=10, in_channels=3)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, loss, optimizer, device, num_epochs):\n",
    "    \"\"\"\n",
    "    Function: \n",
    "        提供训练数据，进行训练。每训练完一个Epoch, 使用验证集进行验证    \n",
    "    \"\"\"\n",
    "    net = net.to(device)\n",
    "    print(\"training on:\", device)\n",
    "    batch_count = 0\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        \n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            ls = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            ls.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_l_sum += ls.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "            \n",
    "        if valid_iter is not None:            \n",
    "            valid_acc = d2l.evaluate_accuracy(valid_iter, net)\n",
    "            valid_output_str = (\"train acc %.5f, valid_acc %.5f,\" % (train_acc_sum / n, valid_acc))\n",
    "        else:\n",
    "            valid_output_str = (\"train acc %.5f, \" % (train_acc_sum / n))\n",
    "\n",
    "        print(\"Epoch %d, loss %.5f, \" % (epoch + 1, train_l_sum / batch_count) + \n",
    "              valid_output_str + \"time %.2f.\" % (time.time() - start))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 500\n",
    "\n",
    "# train(net, train_iter, valid_iter, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda\n",
      "Epoch 1, loss 0.33452, train acc 0.88432, time 31.57.\n",
      "Epoch 2, loss 0.16476, train acc 0.88608, time 30.97.\n",
      "Epoch 3, loss 0.10463, train acc 0.89214, time 30.94.\n",
      "Epoch 4, loss 0.07745, train acc 0.89298, time 31.10.\n",
      "Epoch 5, loss 0.06032, train acc 0.89594, time 31.11.\n",
      "Epoch 6, loss 0.04882, train acc 0.89900, time 32.34.\n",
      "Epoch 7, loss 0.04012, train acc 0.90282, time 31.05.\n",
      "Epoch 8, loss 0.03450, train acc 0.90422, time 31.10.\n",
      "Epoch 9, loss 0.02963, train acc 0.90816, time 31.12.\n",
      "Epoch 10, loss 0.02582, train acc 0.91020, time 31.07.\n",
      "Epoch 11, loss 0.02341, train acc 0.91218, time 31.10.\n",
      "Epoch 12, loss 0.02107, train acc 0.91262, time 31.21.\n",
      "Epoch 13, loss 0.01899, train acc 0.91472, time 31.24.\n",
      "Epoch 14, loss 0.01701, train acc 0.91788, time 31.23.\n",
      "Epoch 15, loss 0.01547, train acc 0.91962, time 30.84.\n",
      "Epoch 16, loss 0.01445, train acc 0.92138, time 31.42.\n",
      "Epoch 17, loss 0.01358, train acc 0.92094, time 30.94.\n",
      "Epoch 18, loss 0.01227, train acc 0.92492, time 30.95.\n",
      "Epoch 19, loss 0.01113, train acc 0.92708, time 30.93.\n",
      "Epoch 20, loss 0.01042, train acc 0.92858, time 30.94.\n",
      "Epoch 21, loss 0.00984, train acc 0.92898, time 31.02.\n",
      "Epoch 22, loss 0.00889, train acc 0.93342, time 31.14.\n",
      "Epoch 23, loss 0.00874, train acc 0.93044, time 31.20.\n",
      "Epoch 24, loss 0.00786, train acc 0.93624, time 31.00.\n",
      "Epoch 25, loss 0.00765, train acc 0.93490, time 30.92.\n",
      "Epoch 26, loss 0.00697, train acc 0.93810, time 30.90.\n",
      "Epoch 27, loss 0.00670, train acc 0.93974, time 30.70.\n",
      "Epoch 28, loss 0.00628, train acc 0.93936, time 31.07.\n",
      "Epoch 29, loss 0.00605, train acc 0.93866, time 30.80.\n",
      "Epoch 30, loss 0.00575, train acc 0.94158, time 30.76.\n",
      "Epoch 31, loss 0.00548, train acc 0.94256, time 30.70.\n",
      "Epoch 32, loss 0.00505, train acc 0.94514, time 30.69.\n",
      "Epoch 33, loss 0.00488, train acc 0.94552, time 30.95.\n",
      "Epoch 34, loss 0.00485, train acc 0.94396, time 31.09.\n",
      "Epoch 35, loss 0.00446, train acc 0.94728, time 30.89.\n",
      "Epoch 36, loss 0.00425, train acc 0.94928, time 30.90.\n",
      "Epoch 37, loss 0.00403, train acc 0.94928, time 31.12.\n",
      "Epoch 38, loss 0.00400, train acc 0.94876, time 31.16.\n",
      "Epoch 39, loss 0.00380, train acc 0.94932, time 31.00.\n",
      "Epoch 40, loss 0.00352, train acc 0.95212, time 30.96.\n",
      "Epoch 41, loss 0.00348, train acc 0.95194, time 30.93.\n",
      "Epoch 42, loss 0.00345, train acc 0.95084, time 31.16.\n",
      "Epoch 43, loss 0.00315, train acc 0.95510, time 31.18.\n",
      "Epoch 44, loss 0.00312, train acc 0.95330, time 31.18.\n",
      "Epoch 45, loss 0.00304, train acc 0.95464, time 31.16.\n",
      "Epoch 46, loss 0.00284, train acc 0.95680, time 31.15.\n",
      "Epoch 47, loss 0.00278, train acc 0.95600, time 30.92.\n",
      "Epoch 48, loss 0.00274, train acc 0.95520, time 30.71.\n",
      "Epoch 49, loss 0.00264, train acc 0.95644, time 30.69.\n",
      "Epoch 50, loss 0.00246, train acc 0.95930, time 30.90.\n",
      "Epoch 51, loss 0.00236, train acc 0.96104, time 30.92.\n",
      "Epoch 52, loss 0.00235, train acc 0.96020, time 30.91.\n",
      "Epoch 53, loss 0.00232, train acc 0.95940, time 30.86.\n",
      "Epoch 54, loss 0.00222, train acc 0.96064, time 30.87.\n",
      "Epoch 55, loss 0.00214, train acc 0.96138, time 30.67.\n",
      "Epoch 56, loss 0.00210, train acc 0.95990, time 30.66.\n",
      "Epoch 57, loss 0.00200, train acc 0.96230, time 30.67.\n",
      "Epoch 58, loss 0.00202, train acc 0.96120, time 31.17.\n",
      "Epoch 59, loss 0.00194, train acc 0.96194, time 30.87.\n",
      "Epoch 60, loss 0.00183, train acc 0.96292, time 30.87.\n",
      "Epoch 61, loss 0.00180, train acc 0.96266, time 30.99.\n",
      "Epoch 62, loss 0.00172, train acc 0.96540, time 30.77.\n",
      "Epoch 63, loss 0.00159, train acc 0.96716, time 30.74.\n",
      "Epoch 64, loss 0.00160, train acc 0.96684, time 30.91.\n",
      "Epoch 65, loss 0.00159, train acc 0.96620, time 31.13.\n",
      "Epoch 66, loss 0.00160, train acc 0.96462, time 30.90.\n",
      "Epoch 67, loss 0.00155, train acc 0.96574, time 31.01.\n",
      "Epoch 68, loss 0.00153, train acc 0.96560, time 30.95.\n",
      "Epoch 69, loss 0.00144, train acc 0.96720, time 31.07.\n",
      "Epoch 70, loss 0.00141, train acc 0.96696, time 30.70.\n",
      "Epoch 71, loss 0.00137, train acc 0.96810, time 30.67.\n",
      "Epoch 72, loss 0.00136, train acc 0.96698, time 30.66.\n",
      "Epoch 73, loss 0.00139, train acc 0.96668, time 30.71.\n",
      "Epoch 74, loss 0.00129, train acc 0.96856, time 30.68.\n",
      "Epoch 75, loss 0.00124, train acc 0.96840, time 30.64.\n",
      "Epoch 76, loss 0.00122, train acc 0.96866, time 30.66.\n",
      "Epoch 77, loss 0.00120, train acc 0.96998, time 30.68.\n",
      "Epoch 78, loss 0.00119, train acc 0.96888, time 30.85.\n",
      "Epoch 79, loss 0.00117, train acc 0.96932, time 31.06.\n",
      "Epoch 80, loss 0.00109, train acc 0.97092, time 30.77.\n",
      "Epoch 81, loss 0.00109, train acc 0.97040, time 30.83.\n",
      "Epoch 82, loss 0.00115, train acc 0.96812, time 30.93.\n",
      "Epoch 83, loss 0.00108, train acc 0.97016, time 30.93.\n",
      "Epoch 84, loss 0.00101, train acc 0.97208, time 31.07.\n",
      "Epoch 85, loss 0.00101, train acc 0.97186, time 30.87.\n",
      "Epoch 86, loss 0.00096, train acc 0.97254, time 30.91.\n",
      "Epoch 87, loss 0.00092, train acc 0.97356, time 30.91.\n",
      "Epoch 88, loss 0.00093, train acc 0.97302, time 30.89.\n",
      "Epoch 89, loss 0.00089, train acc 0.97424, time 30.80.\n",
      "Epoch 90, loss 0.00091, train acc 0.97308, time 30.68.\n",
      "Epoch 91, loss 0.00087, train acc 0.97378, time 30.68.\n",
      "Epoch 92, loss 0.00085, train acc 0.97486, time 30.82.\n",
      "Epoch 93, loss 0.00081, train acc 0.97506, time 30.99.\n",
      "Epoch 94, loss 0.00082, train acc 0.97440, time 31.10.\n",
      "Epoch 95, loss 0.00082, train acc 0.97420, time 31.16.\n",
      "Epoch 96, loss 0.00080, train acc 0.97438, time 30.86.\n",
      "Epoch 97, loss 0.00078, train acc 0.97460, time 30.73.\n",
      "Epoch 98, loss 0.00081, train acc 0.97458, time 30.80.\n",
      "Epoch 99, loss 0.00076, train acc 0.97486, time 30.70.\n",
      "Epoch 100, loss 0.00075, train acc 0.97568, time 30.99.\n",
      "Epoch 101, loss 0.00073, train acc 0.97546, time 30.97.\n",
      "Epoch 102, loss 0.00073, train acc 0.97500, time 31.12.\n",
      "Epoch 103, loss 0.00072, train acc 0.97546, time 30.79.\n",
      "Epoch 104, loss 0.00067, train acc 0.97692, time 30.72.\n",
      "Epoch 105, loss 0.00068, train acc 0.97608, time 31.04.\n",
      "Epoch 106, loss 0.00065, train acc 0.97710, time 30.91.\n",
      "Epoch 107, loss 0.00070, train acc 0.97554, time 30.89.\n",
      "Epoch 108, loss 0.00067, train acc 0.97630, time 30.77.\n",
      "Epoch 109, loss 0.00062, train acc 0.97820, time 30.67.\n",
      "Epoch 110, loss 0.00061, train acc 0.97796, time 30.70.\n",
      "Epoch 111, loss 0.00062, train acc 0.97686, time 30.69.\n",
      "Epoch 112, loss 0.00060, train acc 0.97822, time 30.68.\n",
      "Epoch 113, loss 0.00058, train acc 0.97870, time 30.67.\n",
      "Epoch 114, loss 0.00057, train acc 0.97812, time 30.62.\n",
      "Epoch 115, loss 0.00059, train acc 0.97790, time 30.63.\n",
      "Epoch 116, loss 0.00056, train acc 0.97804, time 30.67.\n",
      "Epoch 117, loss 0.00055, train acc 0.97858, time 30.63.\n",
      "Epoch 118, loss 0.00057, train acc 0.97846, time 30.64.\n",
      "Epoch 119, loss 0.00054, train acc 0.97866, time 30.65.\n",
      "Epoch 120, loss 0.00053, train acc 0.97902, time 30.92.\n",
      "Epoch 121, loss 0.00053, train acc 0.97918, time 31.19.\n",
      "Epoch 122, loss 0.00051, train acc 0.97902, time 32.01.\n",
      "Epoch 123, loss 0.00051, train acc 0.97940, time 31.16.\n",
      "Epoch 124, loss 0.00047, train acc 0.98120, time 31.19.\n",
      "Epoch 125, loss 0.00047, train acc 0.98016, time 30.86.\n",
      "Epoch 126, loss 0.00048, train acc 0.98040, time 30.73.\n",
      "Epoch 127, loss 0.00049, train acc 0.98022, time 30.89.\n",
      "Epoch 128, loss 0.00047, train acc 0.98020, time 30.88.\n",
      "Epoch 129, loss 0.00045, train acc 0.98066, time 30.73.\n",
      "Epoch 130, loss 0.00047, train acc 0.97966, time 30.69.\n",
      "Epoch 131, loss 0.00045, train acc 0.98070, time 30.70.\n",
      "Epoch 132, loss 0.00043, train acc 0.98194, time 30.94.\n",
      "Epoch 133, loss 0.00043, train acc 0.98086, time 31.14.\n",
      "Epoch 134, loss 0.00043, train acc 0.98118, time 31.08.\n",
      "Epoch 135, loss 0.00045, train acc 0.98042, time 31.11.\n",
      "Epoch 136, loss 0.00045, train acc 0.98010, time 31.11.\n",
      "Epoch 137, loss 0.00040, train acc 0.98254, time 31.16.\n",
      "Epoch 138, loss 0.00043, train acc 0.98098, time 30.73.\n",
      "Epoch 139, loss 0.00039, train acc 0.98192, time 30.76.\n",
      "Epoch 140, loss 0.00041, train acc 0.98108, time 30.70.\n",
      "Epoch 141, loss 0.00039, train acc 0.98166, time 30.70.\n",
      "Epoch 142, loss 0.00041, train acc 0.98152, time 30.85.\n",
      "Epoch 143, loss 0.00040, train acc 0.98146, time 30.86.\n",
      "Epoch 144, loss 0.00039, train acc 0.98192, time 30.99.\n",
      "Epoch 145, loss 0.00037, train acc 0.98276, time 31.09.\n",
      "Epoch 146, loss 0.00036, train acc 0.98300, time 31.09.\n",
      "Epoch 147, loss 0.00038, train acc 0.98108, time 31.14.\n",
      "Epoch 148, loss 0.00036, train acc 0.98326, time 31.13.\n",
      "Epoch 149, loss 0.00036, train acc 0.98232, time 30.95.\n",
      "Epoch 150, loss 0.00033, train acc 0.98382, time 30.67.\n",
      "Epoch 151, loss 0.00035, train acc 0.98256, time 30.70.\n",
      "Epoch 152, loss 0.00034, train acc 0.98350, time 30.67.\n",
      "Epoch 153, loss 0.00035, train acc 0.98210, time 30.69.\n",
      "Epoch 154, loss 0.00033, train acc 0.98344, time 30.67.\n",
      "Epoch 155, loss 0.00034, train acc 0.98270, time 30.65.\n",
      "Epoch 156, loss 0.00032, train acc 0.98390, time 30.85.\n",
      "Epoch 157, loss 0.00030, train acc 0.98430, time 30.97.\n",
      "Epoch 158, loss 0.00031, train acc 0.98398, time 31.13.\n",
      "Epoch 159, loss 0.00032, train acc 0.98342, time 31.07.\n",
      "Epoch 160, loss 0.00028, train acc 0.98514, time 31.16.\n",
      "Epoch 161, loss 0.00028, train acc 0.98592, time 30.84.\n",
      "Epoch 162, loss 0.00030, train acc 0.98466, time 30.70.\n",
      "Epoch 163, loss 0.00030, train acc 0.98458, time 31.10.\n",
      "Epoch 164, loss 0.00031, train acc 0.98338, time 30.66.\n",
      "Epoch 165, loss 0.00027, train acc 0.98506, time 30.91.\n",
      "Epoch 166, loss 0.00028, train acc 0.98506, time 30.92.\n",
      "Epoch 167, loss 0.00032, train acc 0.98280, time 30.89.\n",
      "Epoch 168, loss 0.00028, train acc 0.98434, time 30.75.\n",
      "Epoch 169, loss 0.00028, train acc 0.98480, time 30.97.\n",
      "Epoch 170, loss 0.00028, train acc 0.98430, time 30.92.\n",
      "Epoch 171, loss 0.00027, train acc 0.98552, time 30.69.\n",
      "Epoch 172, loss 0.00027, train acc 0.98460, time 30.71.\n",
      "Epoch 173, loss 0.00026, train acc 0.98536, time 30.88.\n",
      "Epoch 174, loss 0.00025, train acc 0.98648, time 30.99.\n",
      "Epoch 175, loss 0.00027, train acc 0.98484, time 30.85.\n",
      "Epoch 176, loss 0.00025, train acc 0.98634, time 30.90.\n",
      "Epoch 177, loss 0.00026, train acc 0.98524, time 30.92.\n",
      "Epoch 178, loss 0.00027, train acc 0.98466, time 30.98.\n",
      "Epoch 179, loss 0.00025, train acc 0.98552, time 30.88.\n",
      "Epoch 180, loss 0.00025, train acc 0.98492, time 30.89.\n",
      "Epoch 181, loss 0.00025, train acc 0.98492, time 30.85.\n",
      "Epoch 182, loss 0.00023, train acc 0.98666, time 30.74.\n",
      "Epoch 183, loss 0.00022, train acc 0.98662, time 30.76.\n",
      "Epoch 184, loss 0.00025, train acc 0.98572, time 30.91.\n",
      "Epoch 185, loss 0.00025, train acc 0.98542, time 30.90.\n",
      "Epoch 186, loss 0.00023, train acc 0.98614, time 30.91.\n",
      "Epoch 187, loss 0.00024, train acc 0.98548, time 30.93.\n",
      "Epoch 188, loss 0.00024, train acc 0.98550, time 30.97.\n",
      "Epoch 189, loss 0.00024, train acc 0.98540, time 31.15.\n",
      "Epoch 190, loss 0.00022, train acc 0.98654, time 30.92.\n",
      "Epoch 191, loss 0.00023, train acc 0.98636, time 30.92.\n",
      "Epoch 192, loss 0.00021, train acc 0.98762, time 31.11.\n",
      "Epoch 193, loss 0.00022, train acc 0.98658, time 31.08.\n",
      "Epoch 194, loss 0.00021, train acc 0.98718, time 31.11.\n",
      "Epoch 195, loss 0.00022, train acc 0.98550, time 30.91.\n",
      "Epoch 196, loss 0.00020, train acc 0.98704, time 30.90.\n",
      "Epoch 197, loss 0.00020, train acc 0.98736, time 31.08.\n",
      "Epoch 198, loss 0.00020, train acc 0.98756, time 31.18.\n",
      "Epoch 199, loss 0.00020, train acc 0.98746, time 31.18.\n",
      "Epoch 200, loss 0.00019, train acc 0.98734, time 30.82.\n",
      "Epoch 201, loss 0.00020, train acc 0.98700, time 30.80.\n",
      "Epoch 202, loss 0.00019, train acc 0.98792, time 30.75.\n",
      "Epoch 203, loss 0.00019, train acc 0.98784, time 30.79.\n",
      "Epoch 204, loss 0.00019, train acc 0.98746, time 31.14.\n",
      "Epoch 205, loss 0.00020, train acc 0.98666, time 31.08.\n",
      "Epoch 206, loss 0.00019, train acc 0.98774, time 31.15.\n",
      "Epoch 207, loss 0.00017, train acc 0.98854, time 31.19.\n",
      "Epoch 208, loss 0.00019, train acc 0.98690, time 31.14.\n",
      "Epoch 209, loss 0.00018, train acc 0.98838, time 30.70.\n",
      "Epoch 210, loss 0.00017, train acc 0.98816, time 30.78.\n",
      "Epoch 211, loss 0.00018, train acc 0.98742, time 30.96.\n",
      "Epoch 212, loss 0.00019, train acc 0.98698, time 30.97.\n",
      "Epoch 213, loss 0.00018, train acc 0.98776, time 30.98.\n",
      "Epoch 214, loss 0.00018, train acc 0.98792, time 30.99.\n",
      "Epoch 215, loss 0.00017, train acc 0.98888, time 30.99.\n",
      "Epoch 216, loss 0.00016, train acc 0.98802, time 30.96.\n",
      "Epoch 217, loss 0.00017, train acc 0.98802, time 31.21.\n",
      "Epoch 218, loss 0.00018, train acc 0.98730, time 30.94.\n",
      "Epoch 219, loss 0.00018, train acc 0.98712, time 30.70.\n",
      "Epoch 220, loss 0.00018, train acc 0.98728, time 30.63.\n",
      "Epoch 221, loss 0.00017, train acc 0.98822, time 30.62.\n",
      "Epoch 222, loss 0.00017, train acc 0.98774, time 30.60.\n",
      "Epoch 223, loss 0.00017, train acc 0.98796, time 30.61.\n",
      "Epoch 224, loss 0.00016, train acc 0.98800, time 30.93.\n",
      "Epoch 225, loss 0.00016, train acc 0.98776, time 31.11.\n",
      "Epoch 226, loss 0.00017, train acc 0.98772, time 31.10.\n",
      "Epoch 227, loss 0.00016, train acc 0.98860, time 31.10.\n",
      "Epoch 228, loss 0.00015, train acc 0.98868, time 31.09.\n",
      "Epoch 229, loss 0.00016, train acc 0.98840, time 31.11.\n",
      "Epoch 230, loss 0.00016, train acc 0.98770, time 30.86.\n",
      "Epoch 231, loss 0.00015, train acc 0.98898, time 31.07.\n",
      "Epoch 232, loss 0.00015, train acc 0.98872, time 30.81.\n",
      "Epoch 233, loss 0.00014, train acc 0.98950, time 31.07.\n",
      "Epoch 234, loss 0.00015, train acc 0.98924, time 31.19.\n",
      "Epoch 235, loss 0.00014, train acc 0.98866, time 31.20.\n",
      "Epoch 236, loss 0.00014, train acc 0.98876, time 31.16.\n",
      "Epoch 237, loss 0.00015, train acc 0.98810, time 30.91.\n",
      "Epoch 238, loss 0.00014, train acc 0.98960, time 30.76.\n",
      "Epoch 239, loss 0.00016, train acc 0.98848, time 31.90.\n",
      "Epoch 240, loss 0.00015, train acc 0.98812, time 31.19.\n",
      "Epoch 241, loss 0.00014, train acc 0.98944, time 30.66.\n",
      "Epoch 242, loss 0.00014, train acc 0.98894, time 30.67.\n",
      "Epoch 243, loss 0.00015, train acc 0.98826, time 30.64.\n",
      "Epoch 244, loss 0.00014, train acc 0.98842, time 30.99.\n",
      "Epoch 245, loss 0.00014, train acc 0.98956, time 31.20.\n",
      "Epoch 246, loss 0.00014, train acc 0.98968, time 30.90.\n",
      "Epoch 247, loss 0.00014, train acc 0.98800, time 30.88.\n",
      "Epoch 248, loss 0.00014, train acc 0.98862, time 30.94.\n",
      "Epoch 249, loss 0.00013, train acc 0.98926, time 30.93.\n",
      "Epoch 250, loss 0.00015, train acc 0.98800, time 30.95.\n",
      "Epoch 251, loss 0.00012, train acc 0.98998, time 30.75.\n",
      "Epoch 252, loss 0.00013, train acc 0.98958, time 30.70.\n",
      "Epoch 253, loss 0.00013, train acc 0.99016, time 30.71.\n",
      "Epoch 254, loss 0.00013, train acc 0.98878, time 31.07.\n",
      "Epoch 255, loss 0.00012, train acc 0.99000, time 30.84.\n",
      "Epoch 256, loss 0.00012, train acc 0.98962, time 30.74.\n",
      "Epoch 257, loss 0.00012, train acc 0.99040, time 30.86.\n",
      "Epoch 258, loss 0.00014, train acc 0.98850, time 31.15.\n",
      "Epoch 259, loss 0.00013, train acc 0.98924, time 31.05.\n",
      "Epoch 260, loss 0.00013, train acc 0.98886, time 30.89.\n",
      "Epoch 261, loss 0.00013, train acc 0.98918, time 30.88.\n",
      "Epoch 262, loss 0.00012, train acc 0.98988, time 31.15.\n",
      "Epoch 263, loss 0.00012, train acc 0.98984, time 30.96.\n",
      "Epoch 264, loss 0.00012, train acc 0.99060, time 30.76.\n",
      "Epoch 265, loss 0.00012, train acc 0.99044, time 30.71.\n",
      "Epoch 266, loss 0.00011, train acc 0.99054, time 30.71.\n",
      "Epoch 267, loss 0.00012, train acc 0.99012, time 30.66.\n",
      "Epoch 268, loss 0.00011, train acc 0.98986, time 30.70.\n",
      "Epoch 269, loss 0.00012, train acc 0.98948, time 30.67.\n",
      "Epoch 270, loss 0.00011, train acc 0.98984, time 30.71.\n",
      "Epoch 271, loss 0.00012, train acc 0.98944, time 30.65.\n",
      "Epoch 272, loss 0.00012, train acc 0.98942, time 30.66.\n",
      "Epoch 273, loss 0.00011, train acc 0.99036, time 30.67.\n",
      "Epoch 274, loss 0.00011, train acc 0.99022, time 30.93.\n",
      "Epoch 275, loss 0.00011, train acc 0.99084, time 31.03.\n",
      "Epoch 276, loss 0.00010, train acc 0.99094, time 30.85.\n",
      "Epoch 277, loss 0.00010, train acc 0.99096, time 31.07.\n",
      "Epoch 278, loss 0.00010, train acc 0.99114, time 31.16.\n",
      "Epoch 279, loss 0.00010, train acc 0.99132, time 30.99.\n",
      "Epoch 280, loss 0.00011, train acc 0.99000, time 31.16.\n",
      "Epoch 281, loss 0.00010, train acc 0.99130, time 31.10.\n",
      "Epoch 282, loss 0.00010, train acc 0.99074, time 30.72.\n",
      "Epoch 283, loss 0.00010, train acc 0.99088, time 31.01.\n",
      "Epoch 284, loss 0.00011, train acc 0.99010, time 30.91.\n",
      "Epoch 285, loss 0.00010, train acc 0.99092, time 30.87.\n",
      "Epoch 286, loss 0.00010, train acc 0.99062, time 30.89.\n",
      "Epoch 287, loss 0.00010, train acc 0.99070, time 30.88.\n",
      "Epoch 288, loss 0.00010, train acc 0.99004, time 30.91.\n",
      "Epoch 289, loss 0.00010, train acc 0.99066, time 30.90.\n",
      "Epoch 290, loss 0.00011, train acc 0.98994, time 30.94.\n",
      "Epoch 291, loss 0.00009, train acc 0.99172, time 30.96.\n",
      "Epoch 292, loss 0.00010, train acc 0.99102, time 30.86.\n",
      "Epoch 293, loss 0.00011, train acc 0.98950, time 30.92.\n",
      "Epoch 294, loss 0.00011, train acc 0.99006, time 31.05.\n",
      "Epoch 295, loss 0.00011, train acc 0.98980, time 31.00.\n",
      "Epoch 296, loss 0.00010, train acc 0.99062, time 30.95.\n",
      "Epoch 297, loss 0.00009, train acc 0.99098, time 30.91.\n",
      "Epoch 298, loss 0.00009, train acc 0.99118, time 30.99.\n",
      "Epoch 299, loss 0.00010, train acc 0.99002, time 31.15.\n",
      "Epoch 300, loss 0.00009, train acc 0.99120, time 31.15.\n",
      "Epoch 301, loss 0.00008, train acc 0.99182, time 30.96.\n",
      "Epoch 302, loss 0.00009, train acc 0.99088, time 30.87.\n",
      "Epoch 303, loss 0.00009, train acc 0.99120, time 30.89.\n",
      "Epoch 304, loss 0.00009, train acc 0.99122, time 30.99.\n",
      "Epoch 305, loss 0.00008, train acc 0.99228, time 30.85.\n",
      "Epoch 306, loss 0.00009, train acc 0.99074, time 30.69.\n",
      "Epoch 307, loss 0.00008, train acc 0.99154, time 30.83.\n",
      "Epoch 308, loss 0.00009, train acc 0.99122, time 30.92.\n",
      "Epoch 309, loss 0.00009, train acc 0.99176, time 30.93.\n",
      "Epoch 310, loss 0.00009, train acc 0.99090, time 30.97.\n",
      "Epoch 311, loss 0.00008, train acc 0.99170, time 31.10.\n",
      "Epoch 312, loss 0.00008, train acc 0.99176, time 31.18.\n",
      "Epoch 313, loss 0.00009, train acc 0.99170, time 31.22.\n",
      "Epoch 314, loss 0.00009, train acc 0.99150, time 30.94.\n",
      "Epoch 315, loss 0.00009, train acc 0.99156, time 30.70.\n",
      "Epoch 316, loss 0.00009, train acc 0.99126, time 30.93.\n",
      "Epoch 317, loss 0.00009, train acc 0.99072, time 31.13.\n",
      "Epoch 318, loss 0.00008, train acc 0.99134, time 31.16.\n",
      "Epoch 319, loss 0.00009, train acc 0.99098, time 31.16.\n",
      "Epoch 320, loss 0.00009, train acc 0.99106, time 31.11.\n",
      "Epoch 321, loss 0.00008, train acc 0.99252, time 30.90.\n",
      "Epoch 322, loss 0.00009, train acc 0.99074, time 30.94.\n",
      "Epoch 323, loss 0.00007, train acc 0.99254, time 30.96.\n",
      "Epoch 324, loss 0.00008, train acc 0.99134, time 31.19.\n",
      "Epoch 325, loss 0.00008, train acc 0.99138, time 31.20.\n",
      "Epoch 326, loss 0.00008, train acc 0.99166, time 31.18.\n",
      "Epoch 327, loss 0.00008, train acc 0.99154, time 31.17.\n",
      "Epoch 328, loss 0.00007, train acc 0.99212, time 31.18.\n",
      "Epoch 329, loss 0.00008, train acc 0.99164, time 31.20.\n",
      "Epoch 330, loss 0.00008, train acc 0.99178, time 31.17.\n",
      "Epoch 331, loss 0.00008, train acc 0.99174, time 31.09.\n",
      "Epoch 332, loss 0.00007, train acc 0.99238, time 31.12.\n",
      "Epoch 333, loss 0.00008, train acc 0.99146, time 31.19.\n",
      "Epoch 334, loss 0.00008, train acc 0.99138, time 31.14.\n",
      "Epoch 335, loss 0.00008, train acc 0.99190, time 30.79.\n",
      "Epoch 336, loss 0.00007, train acc 0.99188, time 30.70.\n",
      "Epoch 337, loss 0.00007, train acc 0.99262, time 30.84.\n",
      "Epoch 338, loss 0.00008, train acc 0.99196, time 31.15.\n",
      "Epoch 339, loss 0.00007, train acc 0.99236, time 30.91.\n",
      "Epoch 340, loss 0.00008, train acc 0.99130, time 31.15.\n",
      "Epoch 341, loss 0.00006, train acc 0.99316, time 31.18.\n",
      "Epoch 342, loss 0.00007, train acc 0.99234, time 31.22.\n",
      "Epoch 343, loss 0.00007, train acc 0.99318, time 31.06.\n",
      "Epoch 344, loss 0.00007, train acc 0.99244, time 30.91.\n",
      "Epoch 345, loss 0.00007, train acc 0.99276, time 30.95.\n",
      "Epoch 346, loss 0.00008, train acc 0.99178, time 30.79.\n",
      "Epoch 347, loss 0.00007, train acc 0.99122, time 30.85.\n",
      "Epoch 348, loss 0.00007, train acc 0.99234, time 31.11.\n",
      "Epoch 349, loss 0.00007, train acc 0.99186, time 30.93.\n",
      "Epoch 350, loss 0.00007, train acc 0.99212, time 30.68.\n",
      "Epoch 351, loss 0.00006, train acc 0.99302, time 30.72.\n",
      "Epoch 352, loss 0.00007, train acc 0.99238, time 31.02.\n",
      "Epoch 353, loss 0.00007, train acc 0.99260, time 31.08.\n",
      "Epoch 354, loss 0.00007, train acc 0.99200, time 30.92.\n",
      "Epoch 355, loss 0.00006, train acc 0.99344, time 31.60.\n",
      "Epoch 356, loss 0.00006, train acc 0.99282, time 30.68.\n",
      "Epoch 357, loss 0.00006, train acc 0.99320, time 31.17.\n",
      "Epoch 358, loss 0.00007, train acc 0.99178, time 31.19.\n",
      "Epoch 359, loss 0.00006, train acc 0.99290, time 30.95.\n",
      "Epoch 360, loss 0.00007, train acc 0.99178, time 30.90.\n",
      "Epoch 361, loss 0.00006, train acc 0.99246, time 30.94.\n",
      "Epoch 362, loss 0.00007, train acc 0.99226, time 31.07.\n",
      "Epoch 363, loss 0.00007, train acc 0.99162, time 31.08.\n",
      "Epoch 364, loss 0.00007, train acc 0.99252, time 30.91.\n",
      "Epoch 365, loss 0.00006, train acc 0.99374, time 30.99.\n",
      "Epoch 366, loss 0.00007, train acc 0.99246, time 30.98.\n",
      "Epoch 367, loss 0.00006, train acc 0.99296, time 31.07.\n",
      "Epoch 368, loss 0.00006, train acc 0.99304, time 31.20.\n",
      "Epoch 369, loss 0.00006, train acc 0.99276, time 30.76.\n",
      "Epoch 370, loss 0.00005, train acc 0.99368, time 30.71.\n",
      "Epoch 371, loss 0.00005, train acc 0.99338, time 31.13.\n",
      "Epoch 372, loss 0.00006, train acc 0.99266, time 30.90.\n",
      "Epoch 373, loss 0.00006, train acc 0.99244, time 30.86.\n",
      "Epoch 374, loss 0.00006, train acc 0.99312, time 30.86.\n",
      "Epoch 375, loss 0.00006, train acc 0.99246, time 30.87.\n",
      "Epoch 376, loss 0.00006, train acc 0.99296, time 30.88.\n",
      "Epoch 377, loss 0.00005, train acc 0.99362, time 30.93.\n",
      "Epoch 378, loss 0.00005, train acc 0.99340, time 30.86.\n",
      "Epoch 379, loss 0.00005, train acc 0.99310, time 30.84.\n",
      "Epoch 380, loss 0.00006, train acc 0.99240, time 30.87.\n",
      "Epoch 381, loss 0.00006, train acc 0.99294, time 30.91.\n",
      "Epoch 382, loss 0.00006, train acc 0.99282, time 30.81.\n",
      "Epoch 383, loss 0.00006, train acc 0.99322, time 31.07.\n",
      "Epoch 384, loss 0.00005, train acc 0.99304, time 31.09.\n",
      "Epoch 385, loss 0.00006, train acc 0.99180, time 31.14.\n",
      "Epoch 386, loss 0.00005, train acc 0.99316, time 31.14.\n",
      "Epoch 387, loss 0.00006, train acc 0.99236, time 30.86.\n",
      "Epoch 388, loss 0.00006, train acc 0.99260, time 30.90.\n",
      "Epoch 389, loss 0.00005, train acc 0.99338, time 31.20.\n",
      "Epoch 390, loss 0.00006, train acc 0.99298, time 31.19.\n",
      "Epoch 391, loss 0.00005, train acc 0.99336, time 31.17.\n",
      "Epoch 392, loss 0.00006, train acc 0.99256, time 31.20.\n",
      "Epoch 393, loss 0.00005, train acc 0.99324, time 31.18.\n",
      "Epoch 394, loss 0.00005, train acc 0.99292, time 31.14.\n",
      "Epoch 395, loss 0.00005, train acc 0.99322, time 30.91.\n",
      "Epoch 396, loss 0.00005, train acc 0.99340, time 30.85.\n",
      "Epoch 397, loss 0.00005, train acc 0.99388, time 30.63.\n",
      "Epoch 398, loss 0.00005, train acc 0.99328, time 30.67.\n",
      "Epoch 399, loss 0.00005, train acc 0.99422, time 30.86.\n",
      "Epoch 400, loss 0.00005, train acc 0.99436, time 31.19.\n",
      "Epoch 401, loss 0.00005, train acc 0.99294, time 30.92.\n",
      "Epoch 402, loss 0.00005, train acc 0.99342, time 30.84.\n",
      "Epoch 403, loss 0.00005, train acc 0.99314, time 30.79.\n",
      "Epoch 404, loss 0.00006, train acc 0.99280, time 30.76.\n",
      "Epoch 405, loss 0.00005, train acc 0.99380, time 30.94.\n",
      "Epoch 406, loss 0.00005, train acc 0.99324, time 30.93.\n",
      "Epoch 407, loss 0.00005, train acc 0.99362, time 30.71.\n",
      "Epoch 408, loss 0.00006, train acc 0.99272, time 30.90.\n",
      "Epoch 409, loss 0.00005, train acc 0.99274, time 30.91.\n",
      "Epoch 410, loss 0.00005, train acc 0.99270, time 30.75.\n",
      "Epoch 411, loss 0.00005, train acc 0.99332, time 31.01.\n",
      "Epoch 412, loss 0.00005, train acc 0.99278, time 30.88.\n",
      "Epoch 413, loss 0.00005, train acc 0.99414, time 30.90.\n",
      "Epoch 414, loss 0.00005, train acc 0.99340, time 30.78.\n",
      "Epoch 415, loss 0.00005, train acc 0.99382, time 31.07.\n",
      "Epoch 416, loss 0.00005, train acc 0.99382, time 31.13.\n",
      "Epoch 417, loss 0.00005, train acc 0.99352, time 31.18.\n",
      "Epoch 418, loss 0.00005, train acc 0.99294, time 31.18.\n",
      "Epoch 419, loss 0.00005, train acc 0.99288, time 30.74.\n",
      "Epoch 420, loss 0.00005, train acc 0.99322, time 30.68.\n",
      "Epoch 421, loss 0.00005, train acc 0.99404, time 30.82.\n",
      "Epoch 422, loss 0.00005, train acc 0.99378, time 30.91.\n",
      "Epoch 423, loss 0.00005, train acc 0.99330, time 31.08.\n",
      "Epoch 424, loss 0.00005, train acc 0.99364, time 31.18.\n",
      "Epoch 425, loss 0.00005, train acc 0.99378, time 31.19.\n",
      "Epoch 426, loss 0.00005, train acc 0.99376, time 31.10.\n",
      "Epoch 427, loss 0.00005, train acc 0.99338, time 30.85.\n",
      "Epoch 428, loss 0.00005, train acc 0.99380, time 30.67.\n",
      "Epoch 429, loss 0.00005, train acc 0.99348, time 30.68.\n",
      "Epoch 430, loss 0.00004, train acc 0.99372, time 30.74.\n",
      "Epoch 431, loss 0.00004, train acc 0.99402, time 30.90.\n",
      "Epoch 432, loss 0.00004, train acc 0.99444, time 30.89.\n",
      "Epoch 433, loss 0.00004, train acc 0.99378, time 30.67.\n",
      "Epoch 434, loss 0.00004, train acc 0.99400, time 30.71.\n",
      "Epoch 435, loss 0.00004, train acc 0.99316, time 30.67.\n",
      "Epoch 436, loss 0.00004, train acc 0.99432, time 30.62.\n",
      "Epoch 437, loss 0.00004, train acc 0.99452, time 30.65.\n",
      "Epoch 438, loss 0.00004, train acc 0.99404, time 30.94.\n",
      "Epoch 439, loss 0.00004, train acc 0.99402, time 30.69.\n",
      "Epoch 440, loss 0.00004, train acc 0.99424, time 30.95.\n",
      "Epoch 441, loss 0.00004, train acc 0.99416, time 30.80.\n",
      "Epoch 442, loss 0.00005, train acc 0.99300, time 31.06.\n",
      "Epoch 443, loss 0.00005, train acc 0.99306, time 31.11.\n",
      "Epoch 444, loss 0.00005, train acc 0.99338, time 30.92.\n",
      "Epoch 445, loss 0.00005, train acc 0.99354, time 30.67.\n",
      "Epoch 446, loss 0.00004, train acc 0.99398, time 30.73.\n",
      "Epoch 447, loss 0.00004, train acc 0.99440, time 30.69.\n",
      "Epoch 448, loss 0.00004, train acc 0.99430, time 30.69.\n",
      "Epoch 449, loss 0.00004, train acc 0.99486, time 30.70.\n",
      "Epoch 450, loss 0.00004, train acc 0.99436, time 30.92.\n",
      "Epoch 451, loss 0.00004, train acc 0.99396, time 30.73.\n",
      "Epoch 452, loss 0.00004, train acc 0.99350, time 30.72.\n",
      "Epoch 453, loss 0.00004, train acc 0.99370, time 30.89.\n",
      "Epoch 454, loss 0.00004, train acc 0.99394, time 30.98.\n",
      "Epoch 455, loss 0.00004, train acc 0.99370, time 31.05.\n",
      "Epoch 456, loss 0.00004, train acc 0.99494, time 31.14.\n",
      "Epoch 457, loss 0.00004, train acc 0.99428, time 31.16.\n",
      "Epoch 458, loss 0.00004, train acc 0.99412, time 31.20.\n",
      "Epoch 459, loss 0.00004, train acc 0.99420, time 31.20.\n",
      "Epoch 460, loss 0.00004, train acc 0.99406, time 30.86.\n",
      "Epoch 461, loss 0.00004, train acc 0.99450, time 30.67.\n",
      "Epoch 462, loss 0.00004, train acc 0.99320, time 30.63.\n",
      "Epoch 463, loss 0.00004, train acc 0.99362, time 30.66.\n",
      "Epoch 464, loss 0.00004, train acc 0.99474, time 30.79.\n",
      "Epoch 465, loss 0.00004, train acc 0.99484, time 31.06.\n",
      "Epoch 466, loss 0.00004, train acc 0.99444, time 30.74.\n",
      "Epoch 467, loss 0.00004, train acc 0.99478, time 30.75.\n",
      "Epoch 468, loss 0.00003, train acc 0.99484, time 30.69.\n",
      "Epoch 469, loss 0.00004, train acc 0.99430, time 30.74.\n",
      "Epoch 470, loss 0.00004, train acc 0.99442, time 30.99.\n",
      "Epoch 471, loss 0.00004, train acc 0.99378, time 31.85.\n",
      "Epoch 472, loss 0.00003, train acc 0.99522, time 30.93.\n",
      "Epoch 473, loss 0.00004, train acc 0.99432, time 30.98.\n",
      "Epoch 474, loss 0.00004, train acc 0.99366, time 30.88.\n",
      "Epoch 475, loss 0.00004, train acc 0.99356, time 30.88.\n",
      "Epoch 476, loss 0.00004, train acc 0.99384, time 30.87.\n",
      "Epoch 477, loss 0.00003, train acc 0.99486, time 30.92.\n",
      "Epoch 478, loss 0.00004, train acc 0.99444, time 30.74.\n",
      "Epoch 479, loss 0.00004, train acc 0.99442, time 30.77.\n",
      "Epoch 480, loss 0.00004, train acc 0.99356, time 30.69.\n",
      "Epoch 481, loss 0.00004, train acc 0.99448, time 30.90.\n",
      "Epoch 482, loss 0.00003, train acc 0.99464, time 30.96.\n",
      "Epoch 483, loss 0.00004, train acc 0.99422, time 30.79.\n",
      "Epoch 484, loss 0.00004, train acc 0.99434, time 30.91.\n",
      "Epoch 485, loss 0.00004, train acc 0.99438, time 30.87.\n",
      "Epoch 486, loss 0.00003, train acc 0.99468, time 30.89.\n",
      "Epoch 487, loss 0.00003, train acc 0.99466, time 30.83.\n",
      "Epoch 488, loss 0.00003, train acc 0.99476, time 30.73.\n",
      "Epoch 489, loss 0.00003, train acc 0.99504, time 30.94.\n",
      "Epoch 490, loss 0.00004, train acc 0.99410, time 30.62.\n",
      "Epoch 491, loss 0.00004, train acc 0.99370, time 30.98.\n",
      "Epoch 492, loss 0.00003, train acc 0.99456, time 31.17.\n",
      "Epoch 493, loss 0.00003, train acc 0.99424, time 31.04.\n",
      "Epoch 494, loss 0.00003, train acc 0.99452, time 30.90.\n",
      "Epoch 495, loss 0.00003, train acc 0.99486, time 30.88.\n",
      "Epoch 496, loss 0.00004, train acc 0.99430, time 30.89.\n",
      "Epoch 497, loss 0.00003, train acc 0.99480, time 30.78.\n",
      "Epoch 498, loss 0.00003, train acc 0.99474, time 30.75.\n",
      "Epoch 499, loss 0.00003, train acc 0.99408, time 30.65.\n",
      "Epoch 500, loss 0.00004, train acc 0.99370, time 30.64.\n"
     ]
    }
   ],
   "source": [
    "# 进入测试环节（用所有训练集训练，所有的测试集进行测试）\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train(net, train_valid_iter, None, loss, optimizer, device, num_epochs)\n",
    "\n",
    "preds = []\n",
    "for X, _ in test_iter:\n",
    "    X = X.to(device)\n",
    "    net = net.to(device)\n",
    "    y_hat = net(X)\n",
    "#     print(y_hat.argmax(dim=1).cpu())\n",
    "    preds.extend(y_hat.argmax(dim=1).cpu())   # 将每个样本的预测值计算出来。预测结果如 0,1,2,...,9\n",
    "\n",
    "sorted_ids = list(range(1, len(test_data) + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label':preds})\n",
    "# df['label'] = df['label'].apply(lambda x: train_val_data.synsets[x])\n",
    "df['label'] = df['label'].apply(lambda x: train_val_data.classes[x])\n",
    "df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 10, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "   id  label\n",
      "0   1      0\n",
      "1  10      2\n",
      "2   2      3\n",
      "3   3      4\n",
      "4   4      5\n",
      "5   5      2\n",
      "6   6      9\n",
      "7   7      9\n",
      "8   8      8\n",
      "9   9      6\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preds = [0, 2, 3, 4, 5, 2, 9, 9, 8, 6]\n",
    "\n",
    "sorted_ids = list(range(1, 10+1))\n",
    "print(sorted_ids)\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "print(sorted_ids)\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label':preds})\n",
    "print(df)\n",
    "\n",
    "\n",
    "unk = train_val_data.classes\n",
    "print(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My first submission. Just a baseline. I will submit in later as the epochs set in a large number, at this time, the epoch is 2."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
